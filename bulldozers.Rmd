---
title: "Blue Book for Bulldozers Kaggle Competition"
output:
   prettydoc::html_pretty:
    keep_md: true
    theme: architect
---

In this notebook I work through the Blue Book for Bulldozers dataset.

Data for and information on this Kaggle competition can be found [here.](https://www.kaggle.com/c/bluebook-for-bulldozers/overview)

```{r Imports, message=FALSE, warning=FALSE}
library(tidyverse)
library(lubridate)
library(visdat)
library(randomForest)
library(caret)
library(e1071)

```
Read in the data and inspect the various data types. As the saldedate variable is represented as a string, we convert it to date-time format using lubridates mdy_hm.  
```{r Read Data, message = FALSE}
PATH = "data/bulldozers/"

data_raw = read_csv(str_c(PATH, "Train.csv"))
data_raw$saledate = mdy_hm(data_raw$saledate)
```
It is apparent that there may be a lot of missing values in our data. To visualize the extent of these missing values we will use vis_miss, then display the proportion of missing values for each column.
```{r Missing Values Viz, message=FALSE}
require(scales)
vis_miss(data_raw, show_perc_col = FALSE, warn_large_data = FALSE) +
  theme(axis.text.x=element_blank(),
        axis.title.x=element_blank()) + 
   scale_y_continuous(labels = comma)
```

```{r Missing Value Proportions}
na_prop = sort(round(colMeans(is.na(data_raw)), 2), decreasing = TRUE)

max_str = max(str_length(names(na_prop)))

for (nm in names(na_prop)) {
    cat(
      str_c(str_pad(str_c(nm, ":"), max_str + 1L, side = "right"),
            na_prop[[nm]],
            sep = " "),
      "\n")
}
```


```{r Numeric Summary}
data_raw %>%
  select_if(is.numeric) %>%
  summary()
```

Upon inspection of the YearMade column, we can see there are some suspicious results. A minimum YearMade of 1000, and a mean YearMade of 1899. We will explore this further by plotting a histogram of YearMade.

```{r YearMade plot, message=FALSE}
ggplot(data_raw, aes(YearMade)) +
  geom_histogram(fill = "#63A0E1") +
   scale_y_continuous(labels = comma)
```




It appears that missing values for YearMade in this data set have been assigned the year 1000. We will set these values to NA and deal with them later.

```{r Fix YearMade, message=FALSE, warning=FALSE}
data_raw$YearMade[data_raw$YearMade < 1900] = NA
ggplot(data_raw, aes(YearMade)) +
  geom_histogram(fill = "#63A0E1", binwidth = 1) +
   scale_y_continuous(labels = comma)
```
```{r}
data_raw %>%
  select_if(is.numeric) %>%
  summary()
```
A minimum YearMade of 1919 and a mean YearMade of 1994 seem much more reasonable. We now perform feature extraction on the sale date, label encode the categorical variables, and address the missing continuous values in the data set. Missing continuous values will be replaced with the median value of that feature, and a new feature will be created to indicate which values were initially missing.


```{r Date Feature Extraction}
data_raw_2 = data_raw %>%
  mutate("Sale_Year" = year(saledate), "Sale_Month" = month(saledate), 
         "Sale_Week" = week(saledate), "Sale_yday" = yday(saledate), 
         "Sale_mday" = mday(saledate), "Sale_wday" = wday(saledate),
         "Sale_Quarter" = quarter(saledate), "Sale_qday" = qday(saledate))
```


```{r Label Encoding}
label_encoder = function(df) {
  for (nm in names(df)) {
    if (typeof(df[[nm]]) == "character") {
      df[[nm]][is.na(df[[nm]])] = "NA"
    }
  }
  for (nm in names(df)) {
    if (typeof(df[[nm]]) == "character") {
      df[[nm]] = as.numeric(as.factor(df[[nm]]))
    }
  }
  return(df)
}

data_raw_3 = label_encoder(data_raw_2)
```

```{r Impute NAs}
data_raw_4 = data_raw_3 %>%
  mutate("auctioneerID_NA" = as.numeric(is.na(data_raw_3$auctioneerID)), 
         "Year_Made_NA" = as.numeric(is.na(data_raw_3$YearMade)),
         "Machine__hours_NA" = as.numeric(is.na(data_raw_3$MachineHoursCurrentMeter))) %>%
  mutate("auctioneerID" = replace(auctioneerID,
                                is.na(auctioneerID),
                                median(auctioneerID, na.rm = TRUE)),
         "YearMade" = replace(YearMade,
                                is.na(YearMade),
                                median(YearMade, na.rm = TRUE)),
         "MachineHoursCurrentMeter" = replace(MachineHoursCurrentMeter,
                                is.na(MachineHoursCurrentMeter),
                                median(MachineHoursCurrentMeter, na.rm = TRUE)))

cat(str_c("There are ", as.character(sum(is.na(data_raw_4))), " missing values remaining."))
```
We split the data into a train and validation set. As we are predicting future sales, the validation set must be comprised of examples of which the sale date is more recent than any example found in the training set. If we randomly select a validation set, then the accuracy on this validation set will not be indicative of the accuracies we may get on future examples, as the validation set will have sale dates that the model has already been trained on.

It is very important that the validation set and the test set come from the same distribution of data, in this case that distribution is a range of dates. We then split off the independent variable, and take its log, as the evaluation metric for this competition is the RMSLE (root mean squared log error) between the actual and predicted auction prices. We will therefore use this metric to score our model.
```{r Train Test Split}
n_valid = 20000
n_train = nrow(data_raw_4) - n_valid

train = data_raw_4 %>%
  arrange(saledate) %>%
  head(n_train)
  
valid = data_raw_4 %>%
  arrange(saledate) %>%
  tail(n_valid)

features_train = train %>%
  select(-SalePrice)

labels_train = train %>%
  select(SalePrice) %>%
  log()

features_valid = valid %>%
  select(-SalePrice)

labels_valid = valid %>%
  select(SalePrice) %>%
  log()
```

```{r Train Model}
# model = randomForest(SalePrice~., data = train, ntree = 150, mtry = 6, importance = TRUE)
```

```{r Save Model}
# saveRDS(model, file = "model.Rdata")
```

```{r Load Model}
model_load = readRDS("model.Rdata")
```


```{r Predictions}
preds = predict(model_load, valid)
```


```{r Error}
cat(str_c("The error on the validation set is " , as.character(format(RMSE(log(preds), 
                                   unlist(labels_valid)),digits = 3, format = "f"))))
```

A RMSLE of 0.281 places us in the top 16% of the leaderboard for this competition. 